{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Links for Scraping\n",
    "\n",
    "This script loops through the 5 different sections of the Stock X website. The intention is just to get the shoe name/colorway key that we would then use in the query to access each shoe's page. There were variables created for each of the sections of the website and the script was ran for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import collections\n",
    "import io\n",
    "import dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 91.0.4472\n",
      "[WDM] - Get LATEST driver version for 91.0.4472\n",
      "[WDM] - Driver [/Users/gabbyvinco/.wdm/drivers/chromedriver/mac64/91.0.4472.19/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install and initiate driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# the start URL from the adidas brand page\n",
    "adidas_url = 'https://stockx.com/adidas'\n",
    "\n",
    "# open and maximize the browser window\n",
    "driver.get(adidas_url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the action object\n",
    "action = webdriver.ActionChains(driver)\n",
    "\n",
    "# get rid of the pop up message upon opening the website\n",
    "wait = WebDriverWait(driver, 10)\n",
    "pop_up_confirm = wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[6]/div[4]/div/section/footer/button')))\n",
    "pop_up_confirm.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty lists to add stuff to iterate through\n",
    "all_xpaths = []\n",
    "all_hrefs = []\n",
    "shoe_link = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create xpath routes for all the links on the page\n",
    "for tile in range (1,21):\n",
    "    # no need to adjust tile path with the change of url\n",
    "    tile_path = \"/html/body/div[1]/div[1]/div[2]/div[2]/div[3]/div/div/div[2]/div[2]/div[2]/div[2]/div[\"+str(tile)+\"]/div/a\"\n",
    "    all_xpaths.append(tile_path)\n",
    "#print(all_xpaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all the href links for each tile on the page\n",
    "def scrape_href():\n",
    "    for path in all_xpaths:\n",
    "        link = driver.find_element_by_xpath(path).get_attribute('href')\n",
    "        all_hrefs.append(link)\n",
    "    #print(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to isolate the link needed for the request\n",
    "def split_href():\n",
    "    for href in all_hrefs:\n",
    "        link = href.split(\"m/\")\n",
    "        shoe_link.append(link[1])\n",
    "    #print(\"links added to list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_list = ['/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/ul/li[2]/a', #jordan tab xpath\n",
    "             '/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/ul/li[3]/a',  #nike tab xpath\n",
    "             '/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/ul/li[4]/a',  #other brands tab xpath\n",
    "             '/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/ul/li[5]/a',  #luxury brands tab xpath\n",
    "             '/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/ul/li[5]/a']  #luxury brands tab xpath \n",
    "             # we have the luxury brands twice so that they will be scraped otherwise the script will cut off before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pass through the different links on the toolbar so then the scraping can \n",
    "# be continuous and not have to be re-ran 5 times\n",
    "def switch_through_tabs(xpath):\n",
    "    browse_tab = driver.find_element_by_xpath('/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/a')\n",
    "    browse_tab.click()\n",
    "    time.sleep(2)\n",
    "    sneaker_tab = driver.find_element_by_xpath('/html/body/div[1]/div[1]/div[1]/nav/div/div/ul/li[1]/ul/li[1]/a')\n",
    "    sneaker_tab.click()\n",
    "    time.sleep(2)\n",
    "    next_tab = driver.find_element_by_xpath(xpath)\n",
    "    next_tab.click()\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if it works before scraping everything\n",
    "# for xpath in xpath_list:\n",
    "#     switch_through_tabs(xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list to hold all the links\n",
    "cumulative_shoe_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working... scraping from this shoe tab.\n",
      "proceed to next shoe tab\n",
      "working... scraping from this shoe tab.\n",
      "proceed to next shoe tab\n",
      "working... scraping from this shoe tab.\n",
      "proceed to next shoe tab\n",
      "working... scraping from this shoe tab.\n",
      "proceed to next shoe tab\n",
      "working... scraping from this shoe tab.\n",
      "proceed to next shoe tab\n",
      "shoe key scrape complete\n"
     ]
    }
   ],
   "source": [
    "for xpath in xpath_list:\n",
    "    # automate it to click through all the pages until we have a duplicate link\n",
    "    page_count = 0\n",
    "    tile_count = 0\n",
    "    # make sure lists are empty before scraping\n",
    "    all_hrefs = []\n",
    "    shoe_link = []\n",
    "\n",
    "    # range 1 to 51 because there are a maximum number of 50 pages to go through\n",
    "    while page_count <= 30:\n",
    "        page_count += 1 # add to count so you know how many iterations you're doing\n",
    "        tile_count += 20 # add to tile count so you can check to see if the current page duplicates with the first page\n",
    "        scrape_href() # get hrefs\n",
    "        split_href() # split hrefs\n",
    "        driver.refresh()\n",
    "        # next button is also the same regardless of switch in url\n",
    "        element = driver.find_element_by_xpath(\"/html/body/div[1]/div[1]/div[2]/div[2]/div[3]/div/div/div[2]/div[3]/ul/a[7]\")\n",
    "        driver.execute_script(\"arguments[0].click();\", element) \n",
    "        # skip the first iteration becuase we know there aren't any duplicates on the first page\n",
    "        if page_count == 1:\n",
    "            print(\"working... scraping from this shoe tab.\")\n",
    "            continue\n",
    "        # if not first iteration, then we need to check for duplicates\n",
    "        elif page_count != 1:\n",
    "            first_shoe = shoe_link[0]\n",
    "            counter=collections.Counter(shoe_link)\n",
    "            unique_shoes = list(counter.keys())\n",
    "            if unique_shoes.count(first_shoe) > 1:\n",
    "                print(\"duplicate found\")\n",
    "                break\n",
    "            else:\n",
    "#                 print(\"working\")\n",
    "                continue\n",
    "    # add shoe_link for this tab to an overall list of all links\n",
    "    cumulative_shoe_links.extend(shoe_link)\n",
    "    print(\"proceed to next shoe tab\")\n",
    "    # switch to the next sneaker tab on the stock x            \n",
    "    switch_through_tabs(xpath)\n",
    "print(\"shoe key scrape complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit from the driver window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49600"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many shoes we scraped (including duplicates)\n",
    "len(cumulative_shoe_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates from scraped shoes\n",
    "counter = collections.Counter(cumulative_shoe_links)\n",
    "all_unique_shoes = list(counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2405"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many shoes we scraped (only unique shoes)\n",
    "len(all_unique_shoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list to df\n",
    "cumulative_list_df = pd.DataFrame(all_unique_shoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv to loop through later in requests\n",
    "with open('cumulative_list.csv', 'w', newline='\\n') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter = csvwriter.writerow(cumulative_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the csv to the project's dropbox folder\n",
    "\n",
    "# running the notebook that contains the twitter api keys to pass the variables in\n",
    "%run ./dropbox_key.ipynb\n",
    "# connect to the group project dropbox\n",
    "DBX = dropbox.Dropbox(dropbox_token)\n",
    "# save df as csv file\n",
    "data = cumulative_list_df.to_csv(index=False) # The index parameter is optional\n",
    "# upload to the project's dropbox folder\n",
    "with io.BytesIO(data.encode()) as stream:\n",
    "    stream.seek(0)\n",
    "    DBX.files_upload(stream.read(), \"/cumulative_list\", mode=dropbox.files.WriteMode.overwrite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
